import os
import json
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup
import urllib3

# ç¦ç”¨å®‰å…¨è­¦å‘Šï¼ˆé’ˆå¯¹é˜¿é‡Œå†…ç½‘ç¯å¢ƒå¸¸è§è¯ä¹¦é—®é¢˜ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def get_desktop_path():
    """è·å–å½“å‰ç”¨æˆ·çš„æ¡Œé¢è·¯å¾„ï¼Œä¼˜å…ˆæ£€æµ‹ OneDrive æ¡Œé¢"""
    user_home = os.path.expanduser("~")
    # å€™é€‰è·¯å¾„ï¼šOneDriveä¸­æ–‡ã€OneDriveè‹±æ–‡ã€æ™®é€šæ¡Œé¢
    candidates = [
        os.path.join(user_home, "OneDrive", "æ¡Œé¢"),
        os.path.join(user_home, "OneDrive", "Desktop"),
        os.path.join(user_home, "Desktop"),
        os.path.join(user_home, "æ¡Œé¢")
    ]
    for path in candidates:
        if os.path.exists(path):
            return path
    return os.path.join(user_home, "Desktop") # é»˜è®¤å…œåº•

# è·å–åŠ¨æ€æ¡Œé¢è·¯å¾„
DESKTOP_PATH = get_desktop_path()


# ==========================================
# ä»»åŠ¡ä¸€ï¼šAE æŠ¥é”€é‚®ä»¶åŒæ­¥ (LAX-AE, ORD-AE, MIA-AE)
# ==========================================
def run_email_sync_task():
    # --- 1. è·¯å¾„ä¸é…ç½® ---
    save_dir = os.path.join(DESKTOP_PATH, "æµ‹è¯•æ–‡ä»¶å¤¹") # ä½¿ç”¨æ£€æµ‹åˆ°çš„æ¡Œé¢è·¯å¾„
    if not os.path.exists(save_dir):
        os.makedirs(save_dir, exist_ok=True)

    TASKS = {
        "101": "LAX-AE.xlsx",
        "102": "ORD-AE.xlsx",
        "106": "MIA-AE.xlsx"
    }

    DIST_MAP = {
        "MAERSK-æœ«ç«¯é…é€": "Maersk",
        "ç¾å›½ç»ŸåŒ…æ ‡å‡†é…é€èµ„æº": "COE",
        "ç¾å›½USPSç›´è¿æœ«ç«¯é…é€": "USPS",
        "SpeedXé…é€": "Speedx",
        "UNIUNIæœ«ç«¯é…é€": "UNIUNI"
    }

    # --- 2. è·å– Token ä¿¡æ¯ ---
    def get_auth():
        user_home = os.path.expanduser("~")
        possible_paths = [
            os.path.join(user_home, "Desktop", "emailtoken.txt"),
            os.path.join(user_home, "OneDrive", "æ¡Œé¢", "emailtoken.txt")
        ]
        token_path = next((p for p in possible_paths if os.path.exists(p)), None)
        if not token_path: return None, None
        try:
            with open(token_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get("cookies", {}), data.get("xsrf_token", None)
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å– Token æ–‡ä»¶: {e}")
            return None, None

    cookies, xsrf_token = get_auth()
    if not xsrf_token:
        print("âŒ AEä»»åŠ¡ï¼šæœªæ‰¾åˆ°æœ‰æ•ˆçš„ emailtoken.txt")
        return

    session = requests.Session()
    session.cookies.update(cookies)
    headers = {"User-Agent": "Mozilla/5.0", "Content-Type": "application/x-www-form-urlencoded"}

    # --- 3. æ ¸å¿ƒè§£æé€»è¾‘ ---
    def parse_mail_body(html_body):
        soup = BeautifulSoup(html_body, "html.parser")
        num_re = re.compile(r'[\d,]+\.?\d*')
        eta_re = re.compile(r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})')

        res = {
            "MAWB": "-", "FLT": "-", "ETA": "-", "T_CTN": 0, "T_PAR": 0, "T_WGT": "-",
            "COE_PAR": "-",  # æ–°å¢ï¼šCOEåŒ…è£¹æ•°å­˜å‚¨
            "W": {k: "-" for k in DIST_MAP.values()},
            "Q": {k: "-" for k in DIST_MAP.values()}
        }

        tables = soup.find_all("table", align="center")
        if not tables: return res

        # ä¸»è¡¨æ•°æ®æå–
        for row in tables[0].find_all("tr"):
            tds = row.find_all("td")
            if len(tds) < 2: continue
            k, v = tds[0].get_text(strip=True), tds[1].get_text(strip=True)
            if "MAWB No." in k:
                res["MAWB"] = v
            elif "Flight Number" in k:
                res["FLT"] = v
            elif "ETA" in k:
                m = eta_re.search(v)
                res["ETA"] = m.group(1) if m else v
            elif "Gross Weight" in k:
                m = num_re.search(v.replace(',', ''))
                res["T_WGT"] = m.group(0) if m else v

        # åˆ†é”€å•†æ˜ç»†æå–
        for table in tables:
            h = table.find("td", class_="distributorName")
            if h:
                dist_name = h.get_text(strip=True)  # ä¿®æ­£ï¼šè·å–åˆ†é”€å•†åç§°ç”¨äºåˆ¤æ–­
                if dist_name in DIST_MAP:
                    col = DIST_MAP[dist_name]
                    for row in table.find_all("tr"):
                        tds = row.find_all("td")
                        if len(tds) < 2: continue
                        label = tds[0].get_text(strip=True)
                        val_raw = tds[1].get_text(strip=True).replace(',', '')
                        m = num_re.search(val_raw)
                        val = float(m.group(0)) if m else 0

                        # ç‰¹æ®Šæå–é€»è¾‘ï¼šå¦‚æœåˆ†é”€å•†æ˜¯COEï¼Œæå–å…¶åŒ…è£¹æ•°
                        if "ç¾å›½ç»ŸåŒ…æ ‡å‡†é…é€èµ„æº" in dist_name and "Quantity of parcels" in label:
                            res["COE_PAR"] = int(val) if val > 0 else "-"

                        if "Gross Weight" in label:
                            res["W"][col] = val if val > 0 else "-"
                        elif "Number of CTN" in label:
                            res["Q"][col] = int(val) if val > 0 else "-"
                            res["T_CTN"] += val
                        elif "Quantity of parcels" in label:
                            res["T_PAR"] += val
        return res

    # --- 4. ä»»åŠ¡æ‰§è¡Œ ---
    for fid, fname in TASKS.items():
        print(f"\nğŸš€ å¼€å§‹å¤„ç† AE ç›®å½• {fid} -> {fname}")
        list_url = "https://mail.alibaba-inc.com/alimail/ajax/mail/queryMailList.txt"
        payload = {"query": f'{{"folderIds":["{fid}"]}}', "showFrom": "1", "offset": "0", "length": "20",
                   "_tpl_": "v5ForWebDing", "_csrf_token_": xsrf_token}

        try:
            r = session.post(list_url, data=payload, headers=headers, verify=False)
            text = r.text
            if text.startswith("/**/"): text = re.search(r'\((.*)\)', text, re.DOTALL).group(1)
            mails = json.loads(text).get("dataList", [])[:20]
        except:
            print(f"  âŒ æ— æ³•è·å–æ–‡ä»¶å¤¹ {fid} çš„é‚®ä»¶åˆ—è¡¨")
            continue

        results = []
        for m in mails:
            mid = m.get("mailId")
            load_url = "https://mail.alibaba-inc.com/alimail/ajax/mail/loadMail.txt"
            try:
                res_json = session.post(load_url, data={"mailId": mid, "full": "1", "_tpl_": "v5ForWebDing",
                                                        "_csrf_token_": xsrf_token}, headers=headers,
                                        verify=False).json()
                if res_json.get("status") in [0, 6]:
                    info = parse_mail_body(res_json["data"]["body"])
                    results.append({
                        "ä¸»å•": info["MAWB"], "èˆªç­å·": info["FLT"], "ETA": info["ETA"],
                        "æ€»ç®±æ•°": int(info["T_CTN"]), "æ€»åŒ…è£¹æ•°": int(info["T_PAR"]), "æ€»é‡é‡": info["T_WGT"],
                        "Maersk é‡é‡": info["W"]["Maersk"], "COE é‡é‡": info["W"]["COE"],
                        "USPS é‡é‡": info["W"]["USPS"],
                        "Speedxé‡é‡": info["W"]["Speedx"], "UNIUNI é‡é‡": info["W"]["UNIUNI"],
                        "Maersk å¤§ç®±æ•°": info["Q"]["Maersk"], "COE å¤§ç®±æ•°": info["Q"]["COE"],
                        "USPS å¤§ç®±æ•°": info["Q"]["USPS"],
                        "Speedxå¤§ç®±æ•°": info["Q"]["Speedx"], "UNIUNI å¤§ç®±æ•°": info["Q"]["UNIUNI"],
                        "COEåŒ…è£¹æ•°": info["COE_PAR"]  # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                    })
            except:
                pass

        if results:
            new_df = pd.DataFrame(results)
            # å¼ºåˆ¶åˆ—åºï¼Œç¡®ä¿ COEåŒ…è£¹æ•° åœ¨æœ€å
            cols = ["ä¸»å•", "èˆªç­å·", "ETA", "æ€»ç®±æ•°", "æ€»åŒ…è£¹æ•°", "æ€»é‡é‡",
                    "Maersk é‡é‡", "COE é‡é‡", "USPS é‡é‡", "Speedxé‡é‡", "UNIUNI é‡é‡",
                    "Maersk å¤§ç®±æ•°", "COE å¤§ç®±æ•°", "USPS å¤§ç®±æ•°", "Speedxå¤§ç®±æ•°", "UNIUNI å¤§ç®±æ•°", "COEåŒ…è£¹æ•°"]
            new_df = new_df.reindex(columns=cols)

            file_path = os.path.join(save_dir, fname)
            if os.path.exists(file_path):
                old_df = pd.read_excel(file_path)
                combined_df = pd.concat([old_df, new_df], ignore_index=True)
            else:
                combined_df = new_df

            # å»é‡ï¼šä¿ç•™éç©ºä¿¡æ¯æœ€å…¨çš„è¡Œ
            combined_df.replace("-", pd.NA, inplace=True)
            target_cols = cols[6:]  # ä»åˆ†é”€å•†æ•°æ®å¼€å§‹ç®—åˆ†
            combined_df['score'] = combined_df[target_cols].notna().sum(axis=1)
            final_df = combined_df.sort_values(by="score", ascending=False).drop_duplicates(
                subset=["ä¸»å•"], keep="first")

            # ä¿å­˜æ—¶ç›´æ¥å¯¼å‡ºï¼Œä¿æŒâ€œæ—§çš„åœ¨ä¸Šï¼Œæ–°çš„åœ¨ä¸‹â€çš„è‡ªç„¶é¡ºåº
            final_df.drop(columns=['score']).fillna("-").to_excel(file_path, index=False)
            print(f"  âœ… å·²æ›´æ–°å¹¶å»é‡ä¿å­˜: {fname}")


# ==========================================
# ä»»åŠ¡äºŒï¼š4PX é‚®ä»¶åŒæ­¥ (LAX-4PX, ORD-4PX, MIA-4PX)
# ==========================================
def run_4px_sync_task():
    save_dir = os.path.join(DESKTOP_PATH, "æµ‹è¯•æ–‡ä»¶å¤¹")
    if not os.path.exists(save_dir): os.makedirs(save_dir, exist_ok=True)

    TASKS = {"104": "LAX-4PX.xlsx", "108": "ORD-4PX.xlsx", "107": "MIA-4PX.xlsx"}
    CHANNEL_ORDER = [
        "COE/UP", "GOFO", "SPX", "TOO", "USPS", "YW EXP", "DHL", "ACI", "FEDEX", "IB", "UNIUNI", "OC UNIUNI",
        "ATL-GOFO", "ATL-USPS", "ATL-ACI", "ATL-DHL"  # <--- å¿…é¡»æŠŠè¿™äº›è¡¥åœ¨è¿™é‡Œï¼
    ]
    EXACT_MATCH_TO_STD = {"GOFO": "GOFO", "TOOEXPRESS": "TOO", "TOOEXPRESS-DF": "TOO", "UNIUNI": "UNIUNI",
                          "OC-UNI": "OC UNIUNI", "DHL": "DHL", "IB": "IB", "SPX": "SPX", "ACI": "ACI",
                          "FEDEX GROUND": "FEDEX", "USPS-GA": "USPS", "COE": "COE/UP", "YW EXPRESS": "YW EXP",
                          "YW EXP": "YW EXP","ATL-GOFO": "ATL-GOFO", "ATL-USPS": "ATL-USPS", "ATL-ACI": "ATL-ACI","ATL-DHL": "ATL-DHL"}

    def get_auth():
        user_home = os.path.expanduser("~")
        possible_paths = [os.path.join(user_home, "Desktop", "emailtoken.txt"),
                          os.path.join(user_home, "OneDrive", "æ¡Œé¢", "emailtoken.txt")]
        token_path = next((p for p in possible_paths if os.path.exists(p)), None)
        if not token_path: return None, None
        try:
            with open(token_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get("cookies", {}), data.get("xsrf_token", None)
        except:
            return None, None

    cookies, xsrf_token = get_auth()
    if not xsrf_token: return

    session = requests.Session()
    session.cookies.update(cookies)
    headers = {"User-Agent": "Mozilla/5.0", "Content-Type": "application/x-www-form-urlencoded"}

    def parse_mail_body(html_body):
        soup = BeautifulSoup(html_body, "html.parser")
        num_re = re.compile(r'[\d,]+\.?\d*')
        eta_re = re.compile(r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})')
        res = {"MAWB": "-", "FLT": "-", "ETA": "-", "T_WGT": "-", "T_PAR": "-", "T_CTN": "-",
               "W": {k: "-" for k in CHANNEL_ORDER}, "Q": {k: "-" for k in CHANNEL_ORDER}}

        tables = soup.find_all("table", align="center")
        if tables:
            for row in tables[0].find_all("tr"):
                tds = row.find_all("td")
                if len(tds) < 2: continue
                k, v = tds[0].get_text(strip=True), tds[1].get_text(strip=True)
                if "MAWB No." in k:
                    res["MAWB"] = v
                elif "Flight Number" in k:
                    res["FLT"] = v
                elif "ETA" in k:
                    m = eta_re.search(v);
                    res["ETA"] = m.group(1) if m else v
                elif "Gross Weight" in k:
                    m = num_re.search(v.replace(',', ''));
                    res["T_WGT"] = m.group(0) if m else v
                elif "Quantity of parcels" in k:
                    m = num_re.search(v.replace(',', ''));
                    res["T_PAR"] = m.group(0) if m else v
                elif "Number of CTN" in k:
                    m = num_re.search(v.replace(',', ''));
                    res["T_CTN"] = m.group(0) if m else v

        all_tables = soup.find_all("table")
        for table in all_tables:
            for row in table.find_all("tr"):
                tds = row.find_all("td")
                if len(tds) < 4: continue
                channel_raw = tds[0].get_text(strip=True)
                if channel_raw in EXACT_MATCH_TO_STD:
                    std_name = EXACT_MATCH_TO_STD[channel_raw]
                    try:
                        ctn_val = num_re.search(tds[1].get_text(strip=True).replace(',', '')).group(0)
                        wgt_val = num_re.search(tds[3].get_text(strip=True).replace(',', '')).group(0)
                        res["Q"][std_name] = ctn_val;
                        res["W"][std_name] = wgt_val
                    except:
                        pass
        return res

    for fid, fname in TASKS.items():
        print(f"ğŸš€ å¤„ç† 4PX ç›®å½• {fid} -> {fname}")
        list_url = "https://mail.alibaba-inc.com/alimail/ajax/mail/queryMailList.txt"
        payload = {"query": f'{{"folderIds":["{fid}"]}}', "showFrom": "1", "offset": "0", "length": "20",
                   "_tpl_": "v5ForWebDing", "_csrf_token_": xsrf_token}
        try:
            r = session.post(list_url, data=payload, headers=headers, verify=False)
            text = r.text
            if text.startswith("/**/"): text = re.search(r'\((.*)\)', text, re.DOTALL).group(1)
            mails = json.loads(text).get("dataList", [])
        except:
            continue

        new_results = []
        for m in mails:
            mid = m.get("mailId")
            load_url = "https://mail.alibaba-inc.com/alimail/ajax/mail/loadMail.txt"
            try:
                mail_json = session.post(load_url, data={"mailId": mid, "full": "1", "_tpl_": "v5ForWebDing",
                                                         "_csrf_token_": xsrf_token}, headers=headers,
                                         verify=False).json()
                if mail_json.get("status") in [0, 6]:
                    info = parse_mail_body(mail_json["data"]["body"])
                    row_data = {"MAWB": info["MAWB"], "Flight Number": info["FLT"], "ETA": info["ETA"],
                                "Weight": info["T_WGT"], "Total Parcels": info["T_PAR"], "Total CTN": info["T_CTN"]}
                    for ch in CHANNEL_ORDER: row_data[f"{ch} Weight"] = info["W"][ch]
                    for ch in CHANNEL_ORDER: row_data[f"{ch} CTN"] = info["Q"][ch]
                    new_results.append(row_data)
            except:
                continue

        if new_results:
            new_df = pd.DataFrame(new_results)
            file_path = os.path.join(save_dir, fname)
            if os.path.exists(file_path):
                old_df = pd.read_excel(file_path)
                combined_df = pd.concat([old_df, new_df], ignore_index=True)
            else:
                combined_df = new_df
            combined_df.replace("-", pd.NA, inplace=True)
            check_cols = [c for c in combined_df.columns if "Weight" in c or "CTN" in c]
            combined_df['score'] = combined_df[check_cols].notna().sum(axis=1)
            final_df = combined_df.sort_values("score", ascending=False).drop_duplicates("MAWB", keep="first")
            # åˆ æ‰ .sort_values("MAWB")ï¼Œæ–°é‚®ä»¶å°±ä¼šä¹–ä¹–å¾…åœ¨ Excel çš„æœ€æœ«å°¾
            final_df.drop(columns=['score']).fillna("-").to_excel(file_path, index=False)
            print(f"  âœ… {fname} å·²æ›´æ–°ã€‚")


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    print("=== å¼€å§‹é‚®ä»¶åŒæ­¥ç¨‹åº ===")
    run_4px_sync_task()
    run_email_sync_task()
    print("=== å…¨éƒ¨åŒæ­¥å®Œæˆ ===")
